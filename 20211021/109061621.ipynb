{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJBKoaDrpWD-"
      },
      "source": [
        "# Week 06: Collocation Extraction\n",
        "In Assignment 5, we found all skip-grams and their frequencies in <u>*wiki1G.txt*</u>. This week, we want to use the result of assignment 5 to extract collocations of [AKL verbs](https://uclouvain.be/en/research-institutes/ilc/cecl/academic-keyword-list.html). We will use [Smadjaâ€™s algorithm](https://aclanthology.org/J93-1007.pdf) to do it. Here are some basic terms need to be explain. \n",
        "\n",
        "We take \"*dpend*\" as an example:\n",
        "\n",
        "<img src=\"https://imgur.com/cPyd7Gr.jpg\" >\n",
        "\n",
        "In this case, we want to find the collocations of \"depend\". Then, \"depend\" is called **base word** and marked as $W$. As for \"on\", \"the\", \"for\"..., they are called **collocate** and marked as $W_{i}$ where **i** represents their serial number. $P_{j}$ means the frequency of $W$ and $W_{i}$ with distance j. And **Freq** is the sum of frequencies of all distances.\n",
        "\n",
        "There are three conditions to filter the skipgram to find collocations. We will go through three conditions below.\n",
        "\n",
        "Considering that some students did not complete Assignment 5, in order to avoid them being unable to do assignment 6, we provide you with a file of calculated skipgram with frequencies, called **AKL_skipgram.tsv**. It only keeps the skipgrams with any AKL verb."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBxDYAFPpWEA"
      },
      "source": [
        "## Read Data\n",
        "<font color=\"red\">**[ TODO ]**</font> Please read <u>*AKL_skipgram.tsv*</u> and store it in the way you like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sVGZSm9fpWEB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "#### here are some hyperparameter\n",
        "k0 = 1\n",
        "k1 = 1\n",
        "U0 = 10\n",
        "base_word = \"depend\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ekEseC_PpWEB"
      },
      "outputs": [],
      "source": [
        "## read file here\n",
        "with open(\"./data/AKL_skipgram.tsv\", \"r\") as f:\n",
        "    collocates = [s.split() for s in f.readlines()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N9bVOqOpWEB"
      },
      "source": [
        "## C1 Condition\n",
        "C1 helps eliminate the collocates that are not frequent enough. This condition specifies that the frequency of appearance of $W_{i}$ in the neighborhood of $W$ must be at least one standard deviation above the average.\n",
        "\n",
        "The formula is here:\n",
        "\n",
        "$$strength = \\frac{freq - \\bar{f}}{\\sigma} \\geq k_{0} = 1$$\n",
        "\n",
        "where $freq$ is the frequency of certain collocate, (e.g., 2573 for \"on\") and \n",
        "\n",
        "$\\bar{f}$ is the average frequencies of all collocates and \n",
        "\n",
        "${\\sigma}$ is the standard deviation of frequencies of all collocates.\n",
        "\n",
        "<font color=\"red\">**[ TODO ]**</font> Please follow the condition to filter the skipgrams of \"depend\" and keep some which pass the condition.\n",
        "\n",
        "The ouput sholud have `collocate` with its `strength`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nQR2AYDxpWEC"
      },
      "outputs": [],
      "source": [
        "def C1_filter(base_word, pt=False):\n",
        "### [TODO]\n",
        "    f_bar, std, filtered_by_C1 = np.mean([int(col[2]) for col in collocates if col[0] == base_word]), np.std([int(col[2]) for col in collocates if col[0] == base_word]), {}\n",
        "    for col in collocates:\n",
        "        if col[0] == base_word:\n",
        "            word, freq = col[1], int(col[2])\n",
        "            strength = round((freq - f_bar) / std, 3)\n",
        "            if strength > k0:\n",
        "                filtered_by_C1[word] = {\"strength\":strength}\n",
        "                if pt: print(\"{}{}\".format(word, filtered_by_C1[word]))\n",
        "\n",
        "    return filtered_by_C1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ikso3UZIpWEC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a{'strength': 6.381}\n",
            "all{'strength': 1.151}\n",
            "also{'strength': 1.133}\n",
            "an{'strength': 1.367}\n",
            "and{'strength': 15.183}\n",
            "are{'strength': 1.962}\n",
            "as{'strength': 2.395}\n",
            "but{'strength': 1.529}\n",
            "by{'strength': 1.042}\n",
            "can{'strength': 1.421}\n",
            "do{'strength': 1.656}\n",
            "does{'strength': 5.299}\n",
            "for{'strength': 4.686}\n",
            "formula{'strength': 1.565}\n",
            "in{'strength': 5.876}\n",
            "is{'strength': 2.611}\n",
            "it{'strength': 2.287}\n",
            "its{'strength': 1.818}\n",
            "may{'strength': 2.864}\n",
            "not{'strength': 8.437}\n",
            "of{'strength': 23.461}\n",
            "on{'strength': 46.313}\n",
            "only{'strength': 1.295}\n",
            "or{'strength': 2.485}\n",
            "other{'strength': 1.656}\n",
            "properties{'strength': 1.042}\n",
            "s{'strength': 2.161}\n",
            "some{'strength': 1.187}\n",
            "such{'strength': 1.439}\n",
            "that{'strength': 7.247}\n",
            "the{'strength': 44.707}\n",
            "their{'strength': 2.828}\n",
            "these{'strength': 1.944}\n",
            "they{'strength': 2.233}\n",
            "this{'strength': 1.908}\n",
            "to{'strength': 8.419}\n",
            "type{'strength': 1.295}\n",
            "upon{'strength': 4.902}\n",
            "which{'strength': 4.379}\n",
            "will{'strength': 3.784}\n",
            "would{'strength': 1.601}\n"
          ]
        }
      ],
      "source": [
        "filtered_by_C1 = C1_filter(base_word, pt=True)\n",
        "### Print"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYguZMfwpWED"
      },
      "source": [
        "<font color=\"green\">Expected output: </font> (The order isn't important.)\n",
        "\n",
        "> a {'strength': 6.381}   \n",
        "> all {'strength': 1.151}   \n",
        "> also {'strength': 1.133}   \n",
        "> an {'strength': 1.367}   \n",
        "> and {'strength': 15.183}   \n",
        "> are {'strength': 1.962}   \n",
        "> as {'strength': 2.395}   \n",
        "> but {'strength': 1.529}   \n",
        "> by {'strength': 1.042}   \n",
        "> can {'strength': 1.421}   \n",
        "> do {'strength': 1.656}   \n",
        "> does {'strength': 5.299}   \n",
        "> for {'strength': 4.686}   \n",
        "> formula {'strength': 1.565}   \n",
        "> in {'strength': 5.876}   \n",
        "> is {'strength': 2.611}   \n",
        "> it {'strength': 2.287}   \n",
        "> its {'strength': 1.818}   \n",
        "> may {'strength': 2.864}   \n",
        "> not {'strength': 8.437}   \n",
        "> of {'strength': 23.461}   \n",
        "> on {'strength': 46.313}   \n",
        "> only {'strength': 1.295}   \n",
        "> or {'strength': 2.485}   \n",
        "> other {'strength': 1.656}   \n",
        "> properties {'strength': 1.042}   \n",
        "> s {'strength': 2.161}   \n",
        "> some {'strength': 1.187}   \n",
        "> such {'strength': 1.439}   \n",
        "> that {'strength': 7.247}   \n",
        "> the {'strength': 44.707}   \n",
        "> their {'strength': 2.828}   \n",
        "> these {'strength': 1.944}   \n",
        "> they {'strength': 2.233}   \n",
        "> this {'strength': 1.908}   \n",
        "> to {'strength': 8.419}   \n",
        "> type {'strength': 1.295}   \n",
        "> upon {'strength': 4.902}   \n",
        "> which {'strength': 4.379}   \n",
        "> will {'strength': 3.784}   \n",
        "> would {'strength': 1.601}   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lWRmQrGpWED"
      },
      "source": [
        "## C2 Condition\n",
        "C2 requires that the histogram of the 10 relative frequencies of appearance of $W_i$ within five words of $W$ (or $p^j_i$s) have at least one spike. If the histogram is flat, it will be rejected by this condition.\n",
        "\n",
        "The formula is here:\n",
        "\n",
        "$$spread = \\frac{\\Sigma^{10}_{j=1}(p^j_i - \\bar{p_i})^2}{10} \\geq U_{0} = 10$$\n",
        "\n",
        "where $p^j_i$ is the frequency of certain collocate with a distance of *j*, (e.g., 16 for \"on\" when its distance is -5) and \n",
        "\n",
        "$\\bar{p_i}$ is the average frequencies of \"on\" with any distance \n",
        "\n",
        "<font color=\"red\">**[ TODO ]**</font> Please follow C2 to filter the result of C1 and keep some which pass C2.\n",
        "\n",
        "The ouput sholud have `collocate` with `strength` and `spread`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "s2p0LbZtpWEE"
      },
      "outputs": [],
      "source": [
        "def C2_filter(base_word, filtered_by_C1, pt=False):\n",
        "### [TODO]\n",
        "    filtered_by_C2 = {}\n",
        "    for col in collocates:\n",
        "        if col[0] == base_word and col[1] in filtered_by_C1:\n",
        "            word, pi_bar = col[1], int(col[2]) / 10\n",
        "            up = [(int(col[i]) - pi_bar)**2 for i in range(3, 13)]\n",
        "            spread = round(sum(up) / 10, 2)\n",
        "            if spread >= U0:\n",
        "                filtered_by_C2[word] = {\"strength\":filtered_by_C1[word][\"strength\"], \"spread\":spread}\n",
        "                if pt: print(\"{}{}\".format(word, filtered_by_C2[word]))\n",
        "    \n",
        "    return filtered_by_C2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OJPDVL0WpWEE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a{'strength': 6.381, 'spread': 777.29}\n",
            "all{'strength': 1.151, 'spread': 29.89}\n",
            "also{'strength': 1.133, 'spread': 208.96}\n",
            "an{'strength': 1.367, 'spread': 56.29}\n",
            "and{'strength': 15.183, 'spread': 2170.41}\n",
            "are{'strength': 1.962, 'spread': 98.84}\n",
            "as{'strength': 2.395, 'spread': 104.96}\n",
            "but{'strength': 1.529, 'spread': 24.4}\n",
            "by{'strength': 1.042, 'spread': 26.21}\n",
            "can{'strength': 1.421, 'spread': 208.24}\n",
            "do{'strength': 1.656, 'spread': 410.21}\n",
            "does{'strength': 5.299, 'spread': 6477.09}\n",
            "for{'strength': 4.686, 'spread': 376.65}\n",
            "formula{'strength': 1.565, 'spread': 46.16}\n",
            "in{'strength': 5.876, 'spread': 396.09}\n",
            "is{'strength': 2.611, 'spread': 148.2}\n",
            "it{'strength': 2.287, 'spread': 112.76}\n",
            "its{'strength': 1.818, 'spread': 94.24}\n",
            "may{'strength': 2.864, 'spread': 1352.24}\n",
            "not{'strength': 8.437, 'spread': 12938.41}\n",
            "of{'strength': 23.461, 'spread': 20132.64}\n",
            "on{'strength': 46.313, 'spread': 420371.01}\n",
            "only{'strength': 1.295, 'spread': 134.01}\n",
            "or{'strength': 2.485, 'spread': 85.61}\n",
            "other{'strength': 1.656, 'spread': 31.61}\n",
            "properties{'strength': 1.042, 'spread': 30.21}\n",
            "s{'strength': 2.161, 'spread': 125.85}\n",
            "some{'strength': 1.187, 'spread': 15.29}\n",
            "such{'strength': 1.439, 'spread': 27.45}\n",
            "that{'strength': 7.247, 'spread': 1492.61}\n",
            "the{'strength': 44.707, 'spread': 98586.04}\n",
            "their{'strength': 2.828, 'spread': 209.56}\n",
            "these{'strength': 1.944, 'spread': 180.01}\n",
            "they{'strength': 2.233, 'spread': 316.09}\n",
            "this{'strength': 1.908, 'spread': 71.09}\n",
            "to{'strength': 8.419, 'spread': 3941.16}\n",
            "type{'strength': 1.295, 'spread': 213.41}\n",
            "upon{'strength': 4.902, 'spread': 4984.01}\n",
            "which{'strength': 4.379, 'spread': 346.16}\n",
            "will{'strength': 3.784, 'spread': 2250.05}\n",
            "would{'strength': 1.601, 'spread': 412.44}\n"
          ]
        }
      ],
      "source": [
        "filtered_by_C2 = C2_filter(base_word, filtered_by_C1, pt=True)\n",
        "### Print"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blC4RZwwpWEE"
      },
      "source": [
        "<font color=\"green\">Expected output: </font> (The order isn't important.)\n",
        "\n",
        "> a {'strength': 6.381, 'spread': 777.29}   \n",
        "> all {'strength': 1.151, 'spread': 29.89}   \n",
        "> also {'strength': 1.133, 'spread': 208.96}   \n",
        "> an {'strength': 1.367, 'spread': 56.29}   \n",
        "> and {'strength': 15.183, 'spread': 2170.41}   \n",
        "> are {'strength': 1.962, 'spread': 98.84}   \n",
        "> as {'strength': 2.395, 'spread': 104.96}   \n",
        "> but {'strength': 1.529, 'spread': 24.4}   \n",
        "> by {'strength': 1.042, 'spread': 26.21}   \n",
        "> can {'strength': 1.421, 'spread': 208.24}   \n",
        "> do {'strength': 1.656, 'spread': 410.21}   \n",
        "> does {'strength': 5.299, 'spread': 6477.09}   \n",
        "> for {'strength': 4.686, 'spread': 376.65}   \n",
        "> formula {'strength': 1.565, 'spread': 46.16}   \n",
        "> in {'strength': 5.876, 'spread': 396.09}   \n",
        "> is {'strength': 2.611, 'spread': 148.2}   \n",
        "> it {'strength': 2.287, 'spread': 112.76}   \n",
        "> its {'strength': 1.818, 'spread': 94.24}   \n",
        "> may {'strength': 2.864, 'spread': 1352.24}   \n",
        "> not {'strength': 8.437, 'spread': 12938.41}   \n",
        "> of {'strength': 23.461, 'spread': 20132.64}   \n",
        "> on {'strength': 46.313, 'spread': 420371.01}   \n",
        "> only {'strength': 1.295, 'spread': 134.01}   \n",
        "> or {'strength': 2.485, 'spread': 85.61}   \n",
        "> other {'strength': 1.656, 'spread': 31.61}   \n",
        "> properties {'strength': 1.042, 'spread': 30.21}   \n",
        "> s {'strength': 2.161, 'spread': 125.85}   \n",
        "> some {'strength': 1.187, 'spread': 15.29}   \n",
        "> such {'strength': 1.439, 'spread': 27.45}   \n",
        "> that {'strength': 7.247, 'spread': 1492.61}   \n",
        "> the {'strength': 44.707, 'spread': 98586.04}   \n",
        "> their {'strength': 2.828, 'spread': 209.56}   \n",
        "> these {'strength': 1.944, 'spread': 180.01}   \n",
        "> they {'strength': 2.233, 'spread': 316.09}   \n",
        "> this {'strength': 1.908, 'spread': 71.09}   \n",
        "> to {'strength': 8.419, 'spread': 3941.16}   \n",
        "> type {'strength': 1.295, 'spread': 213.41}   \n",
        "> upon {'strength': 4.902, 'spread': 4984.01}   \n",
        "> which {'strength': 4.379, 'spread': 346.16}   \n",
        "> will {'strength': 3.784, 'spread': 2250.05}   \n",
        "> would {'strength': 1.601, 'spread': 412.44}   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naS9EpYZpWEF"
      },
      "source": [
        "## C3 Condition\n",
        "C3 keeps the interesting collocates by pulling out the peaks of the $p^j_i$ distributions.\n",
        "\n",
        "Formula:\n",
        "\n",
        "$$p^j_i \\geq \\bar{p_i} + (k_1 \\times \\sqrt{U_{i}})$$\n",
        "\n",
        "where $U_i$ is *spread* in C2 and\n",
        "\n",
        "$k_1$ is equal to 1 \n",
        "\n",
        "<font color=\"red\">**[ TODO ]**</font> Please follow the condition to filter the result of last step and keep some which pass C3.\n",
        "\n",
        "The ouput sholud have `base word, collocate, distance, strength, spread, peak, count`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1aHJ7GHlpWEF"
      },
      "outputs": [],
      "source": [
        "def C3_filter(base_word, filtered_by_C2, pt=False):\n",
        "### [TODO]\n",
        "    filtered_by_C3, step = {}, {3:-5, 4:-4, 5:-3, 6:-2, 7:-1, 8:1, 9:2, 10:3, 11:4, 12:5}\n",
        "    for col in collocates:\n",
        "        if col[0] == base_word and col[1] in filtered_by_C2:\n",
        "            word, pi_bar, spread = col[1], int(col[2]) / 10, filtered_by_C2[col[1]][\"spread\"]\n",
        "            criterion = round(pi_bar + (k1 * np.sqrt(spread)), 3)\n",
        "            for i in range(3, 13):\n",
        "                pij = int(col[i])\n",
        "                if pij >= criterion:\n",
        "                    filtered_by_C3[(base_word, word, step[i])] = {\"strength\":filtered_by_C2[word][\"strength\"], \"spread\":filtered_by_C2[word][\"spread\"], \"peak\":criterion, \"count\":pij}\n",
        "                    if pt: print(\"{}{}\".format((base_word, word, step[i]), filtered_by_C3[(base_word, word, step[i])]))\n",
        "    \n",
        "    return filtered_by_C3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bFNox2TYpWEF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('depend', 'a', 2){'strength': 6.381, 'spread': 777.29, 'peak': 63.78, 'count': 94}\n",
            "('depend', 'all', -4){'strength': 1.151, 'spread': 29.89, 'peak': 12.367, 'count': 14}\n",
            "('depend', 'all', -3){'strength': 1.151, 'spread': 29.89, 'peak': 12.367, 'count': 16}\n",
            "('depend', 'also', -1){'strength': 1.133, 'spread': 208.96, 'peak': 21.255, 'count': 50}\n",
            "('depend', 'an', 2){'strength': 1.367, 'spread': 56.29, 'peak': 15.603, 'count': 24}\n",
            "('depend', 'an', 5){'strength': 1.367, 'spread': 56.29, 'peak': 15.603, 'count': 19}\n",
            "('depend', 'and', 4){'strength': 15.183, 'spread': 2170.41, 'peak': 131.288, 'count': 149}\n",
            "('depend', 'are', -5){'strength': 1.962, 'spread': 98.84, 'peak': 21.342, 'count': 27}\n",
            "('depend', 'are', -4){'strength': 1.962, 'spread': 98.84, 'peak': 21.342, 'count': 22}\n",
            "('depend', 'as', 4){'strength': 2.395, 'spread': 104.96, 'peak': 24.045, 'count': 30}\n",
            "('depend', 'as', 5){'strength': 2.395, 'spread': 104.96, 'peak': 24.045, 'count': 28}\n",
            "('depend', 'but', -2){'strength': 1.529, 'spread': 24.4, 'peak': 13.94, 'count': 14}\n",
            "('depend', 'but', 5){'strength': 1.529, 'spread': 24.4, 'peak': 13.94, 'count': 15}\n",
            "('depend', 'by', -5){'strength': 1.042, 'spread': 26.21, 'peak': 11.42, 'count': 13}\n",
            "('depend', 'by', -4){'strength': 1.042, 'spread': 26.21, 'peak': 11.42, 'count': 12}\n",
            "('depend', 'by', 4){'strength': 1.042, 'spread': 26.21, 'peak': 11.42, 'count': 13}\n",
            "('depend', 'can', -1){'strength': 1.421, 'spread': 208.24, 'peak': 22.831, 'count': 49}\n",
            "('depend', 'do', -2){'strength': 1.656, 'spread': 410.21, 'peak': 29.954, 'count': 70}\n",
            "('depend', 'does', -2){'strength': 5.299, 'spread': 6477.09, 'peak': 110.38, 'count': 271}\n",
            "('depend', 'for', 4){'strength': 4.686, 'spread': 376.65, 'peak': 45.907, 'count': 69}\n",
            "('depend', 'formula', -4){'strength': 1.565, 'spread': 46.16, 'peak': 15.994, 'count': 19}\n",
            "('depend', 'formula', 2){'strength': 1.565, 'spread': 46.16, 'peak': 15.994, 'count': 17}\n",
            "('depend', 'formula', 5){'strength': 1.565, 'spread': 46.16, 'peak': 15.994, 'count': 19}\n",
            "('depend', 'in', -5){'strength': 5.876, 'spread': 396.09, 'peak': 53.002, 'count': 55}\n",
            "('depend', 'in', 4){'strength': 5.876, 'spread': 396.09, 'peak': 53.002, 'count': 62}\n",
            "('depend', 'is', -5){'strength': 2.611, 'spread': 148.2, 'peak': 27.174, 'count': 37}\n",
            "('depend', 'is', 5){'strength': 2.611, 'spread': 148.2, 'peak': 27.174, 'count': 29}\n",
            "('depend', 'it', -3){'strength': 2.287, 'spread': 112.76, 'peak': 23.819, 'count': 39}\n",
            "('depend', 'it', -2){'strength': 2.287, 'spread': 112.76, 'peak': 23.819, 'count': 24}\n",
            "('depend', 'its', 2){'strength': 1.818, 'spread': 94.24, 'peak': 20.308, 'count': 36}\n",
            "('depend', 'may', -1){'strength': 2.864, 'spread': 1352.24, 'peak': 53.173, 'count': 126}\n",
            "('depend', 'not', -1){'strength': 8.437, 'spread': 12938.41, 'peak': 161.047, 'count': 388}\n",
            "('depend', 'of', 4){'strength': 23.461, 'spread': 20132.64, 'peak': 272.49, 'count': 495}\n",
            "('depend', 'on', 1){'strength': 46.313, 'spread': 420371.01, 'peak': 905.66, 'count': 2195}\n",
            "('depend', 'only', 1){'strength': 1.295, 'spread': 134.01, 'peak': 19.276, 'count': 40}\n",
            "('depend', 'or', 4){'strength': 2.485, 'spread': 85.61, 'peak': 23.553, 'count': 29}\n",
            "('depend', 'or', 5){'strength': 2.485, 'spread': 85.61, 'peak': 23.553, 'count': 25}\n",
            "('depend', 'other', 3){'strength': 1.656, 'spread': 31.61, 'peak': 15.322, 'count': 19}\n",
            "('depend', 'other', 5){'strength': 1.656, 'spread': 31.61, 'peak': 15.322, 'count': 17}\n",
            "('depend', 'properties', -4){'strength': 1.042, 'spread': 30.21, 'peak': 11.796, 'count': 12}\n",
            "('depend', 'properties', -1){'strength': 1.042, 'spread': 30.21, 'peak': 11.796, 'count': 15}\n",
            "('depend', 'properties', 3){'strength': 1.042, 'spread': 30.21, 'peak': 11.796, 'count': 15}\n",
            "('depend', 's', 4){'strength': 2.161, 'spread': 125.85, 'peak': 23.718, 'count': 41}\n",
            "('depend', 'some', -3){'strength': 1.187, 'spread': 15.29, 'peak': 11.01, 'count': 13}\n",
            "('depend', 'some', 2){'strength': 1.187, 'spread': 15.29, 'peak': 11.01, 'count': 14}\n",
            "('depend', 'such', 4){'strength': 1.439, 'spread': 27.45, 'peak': 13.739, 'count': 17}\n",
            "('depend', 'that', -3){'strength': 7.247, 'spread': 1492.61, 'peak': 79.334, 'count': 84}\n",
            "('depend', 'that', -1){'strength': 7.247, 'spread': 1492.61, 'peak': 79.334, 'count': 132}\n",
            "('depend', 'the', 2){'strength': 44.707, 'spread': 98586.04, 'peak': 562.384, 'count': 1140}\n",
            "('depend', 'their', 2){'strength': 2.828, 'spread': 209.56, 'peak': 30.676, 'count': 52}\n",
            "('depend', 'these', -2){'strength': 1.944, 'spread': 180.01, 'peak': 24.717, 'count': 48}\n",
            "('depend', 'they', -1){'strength': 2.233, 'spread': 316.09, 'peak': 30.679, 'count': 63}\n",
            "('depend', 'this', -4){'strength': 1.908, 'spread': 71.09, 'peak': 19.531, 'count': 28}\n",
            "('depend', 'this', -2){'strength': 1.908, 'spread': 71.09, 'peak': 19.531, 'count': 22}\n",
            "('depend', 'to', -1){'strength': 8.419, 'spread': 3941.16, 'peak': 109.979, 'count': 228}\n",
            "('depend', 'type', 3){'strength': 1.295, 'spread': 213.41, 'peak': 22.309, 'count': 50}\n",
            "('depend', 'upon', 1){'strength': 4.902, 'spread': 4984.01, 'peak': 98.298, 'count': 239}\n",
            "('depend', 'which', -1){'strength': 4.379, 'spread': 346.16, 'peak': 43.405, 'count': 66}\n",
            "('depend', 'will', -1){'strength': 3.784, 'spread': 2250.05, 'peak': 68.935, 'count': 159}\n",
            "('depend', 'would', -1){'strength': 1.601, 'spread': 412.44, 'peak': 29.709, 'count': 70}\n"
          ]
        }
      ],
      "source": [
        "filtered_by_C3 = C3_filter(base_word, filtered_by_C2, pt=True)\n",
        "### Print"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9K43nWOqpWEF"
      },
      "source": [
        "<font color=\"green\">Expected output: </font> (The order isn't important.)\n",
        "\n",
        "> ('depend', 'a', 2) {'strength': 6.381, 'spread': 777.29, 'peak': 63.78, 'count': 94}   \n",
        "> ('depend', 'all', -4) {'strength': 1.151, 'spread': 29.89, 'peak': 12.367, 'count': 14}   \n",
        "> ('depend', 'all', -3) {'strength': 1.151, 'spread': 29.89, 'peak': 12.367, 'count': 16}   \n",
        "> ('depend', 'also', -1) {'strength': 1.133, 'spread': 208.96, 'peak': 21.255, 'count': 50}   \n",
        "> ('depend', 'an', 2) {'strength': 1.367, 'spread': 56.29, 'peak': 15.603, 'count': 24}   \n",
        "> ('depend', 'an', 5) {'strength': 1.367, 'spread': 56.29, 'peak': 15.603, 'count': 19}   \n",
        "> ('depend', 'and', 4) {'strength': 15.183, 'spread': 2170.41, 'peak': 131.288, 'count': 149}   \n",
        "> ('depend', 'are', -5) {'strength': 1.962, 'spread': 98.84, 'peak': 21.342, 'count': 27}   \n",
        "> ('depend', 'are', -4) {'strength': 1.962, 'spread': 98.84, 'peak': 21.342, 'count': 22}   \n",
        "> ('depend', 'as', 4) {'strength': 2.395, 'spread': 104.96, 'peak': 24.045, 'count': 30}   \n",
        "> ('depend', 'as', 5) {'strength': 2.395, 'spread': 104.96, 'peak': 24.045, 'count': 28}   \n",
        "> ('depend', 'but', -2) {'strength': 1.529, 'spread': 24.4, 'peak': 13.94, 'count': 14}   \n",
        "> ('depend', 'but', 5) {'strength': 1.529, 'spread': 24.4, 'peak': 13.94, 'count': 15}   \n",
        "> ('depend', 'by', -5) {'strength': 1.042, 'spread': 26.21, 'peak': 11.42, 'count': 13}   \n",
        "> ('depend', 'by', -4) {'strength': 1.042, 'spread': 26.21, 'peak': 11.42, 'count': 12}   \n",
        "> ('depend', 'by', 4) {'strength': 1.042, 'spread': 26.21, 'peak': 11.42, 'count': 13}   \n",
        "> ('depend', 'can', -1) {'strength': 1.421, 'spread': 208.24, 'peak': 22.831, 'count': 49}   \n",
        "> ('depend', 'do', -2) {'strength': 1.656, 'spread': 410.21, 'peak': 29.954, 'count': 70}   \n",
        "> ('depend', 'does', -2) {'strength': 5.299, 'spread': 6477.09, 'peak': 110.38, 'count': 271}   \n",
        "> ('depend', 'for', 4) {'strength': 4.686, 'spread': 376.65, 'peak': 45.907, 'count': 69}   \n",
        "> ('depend', 'formula', -4) {'strength': 1.565, 'spread': 46.16, 'peak': 15.994, 'count': 19}   \n",
        "> ('depend', 'formula', 2) {'strength': 1.565, 'spread': 46.16, 'peak': 15.994, 'count': 17}   \n",
        "> ('depend', 'formula', 5) {'strength': 1.565, 'spread': 46.16, 'peak': 15.994, 'count': 19}   \n",
        "> ('depend', 'in', -5) {'strength': 5.876, 'spread': 396.09, 'peak': 53.002, 'count': 55}   \n",
        "> ('depend', 'in', 4) {'strength': 5.876, 'spread': 396.09, 'peak': 53.002, 'count': 62}   \n",
        "> ('depend', 'is', -5) {'strength': 2.611, 'spread': 148.2, 'peak': 27.174, 'count': 37}   \n",
        "> ('depend', 'is', 5) {'strength': 2.611, 'spread': 148.2, 'peak': 27.174, 'count': 29}   \n",
        "> ('depend', 'it', -3) {'strength': 2.287, 'spread': 112.76, 'peak': 23.819, 'count': 39}   \n",
        "> ('depend', 'it', -2) {'strength': 2.287, 'spread': 112.76, 'peak': 23.819, 'count': 24}   \n",
        "> ('depend', 'its', 2) {'strength': 1.818, 'spread': 94.24, 'peak': 20.308, 'count': 36}   \n",
        "> ('depend', 'may', -1) {'strength': 2.864, 'spread': 1352.24, 'peak': 53.173, 'count': 126}   \n",
        "> ('depend', 'not', -1) {'strength': 8.437, 'spread': 12938.41, 'peak': 161.047, 'count': 388}   \n",
        "> ('depend', 'of', 4) {'strength': 23.461, 'spread': 20132.64, 'peak': 272.49, 'count': 495}   \n",
        "> ('depend', 'on', 1) {'strength': 46.313, 'spread': 420371.01, 'peak': 905.66, 'count': 2195}   \n",
        "> ('depend', 'only', 1) {'strength': 1.295, 'spread': 134.01, 'peak': 19.276, 'count': 40}   \n",
        "> ('depend', 'or', 4) {'strength': 2.485, 'spread': 85.61, 'peak': 23.553, 'count': 29}   \n",
        "> ('depend', 'or', 5) {'strength': 2.485, 'spread': 85.61, 'peak': 23.553, 'count': 25}   \n",
        "> ('depend', 'other', 3) {'strength': 1.656, 'spread': 31.61, 'peak': 15.322, 'count': 19}   \n",
        "> ('depend', 'other', 5) {'strength': 1.656, 'spread': 31.61, 'peak': 15.322, 'count': 17}   \n",
        "> ('depend', 'properties', -4) {'strength': 1.042, 'spread': 30.21, 'peak': 11.796, 'count': 12}   \n",
        "> ('depend', 'properties', -1) {'strength': 1.042, 'spread': 30.21, 'peak': 11.796, 'count': 15}   \n",
        "> ('depend', 'properties', 3) {'strength': 1.042, 'spread': 30.21, 'peak': 11.796, 'count': 15}   \n",
        "> ('depend', 's', 4) {'strength': 2.161, 'spread': 125.85, 'peak': 23.718, 'count': 41}   \n",
        "> ('depend', 'some', -3) {'strength': 1.187, 'spread': 15.29, 'peak': 11.01, 'count': 13}   \n",
        "> ('depend', 'some', 2) {'strength': 1.187, 'spread': 15.29, 'peak': 11.01, 'count': 14}   \n",
        "> ('depend', 'such', 4) {'strength': 1.439, 'spread': 27.45, 'peak': 13.739, 'count': 17}   \n",
        "> ('depend', 'that', -3) {'strength': 7.247, 'spread': 1492.61, 'peak': 79.334, 'count': 84}   \n",
        "> ('depend', 'that', -1) {'strength': 7.247, 'spread': 1492.61, 'peak': 79.334, 'count': 132}   \n",
        "> ('depend', 'the', 2) {'strength': 44.707, 'spread': 98586.04, 'peak': 562.384, 'count': 1140}   \n",
        "> ('depend', 'their', 2) {'strength': 2.828, 'spread': 209.56, 'peak': 30.676, 'count': 52}   \n",
        "> ('depend', 'these', -2) {'strength': 1.944, 'spread': 180.01, 'peak': 24.717, 'count': 48}   \n",
        "> ('depend', 'they', -1) {'strength': 2.233, 'spread': 316.09, 'peak': 30.679, 'count': 63}   \n",
        "> ('depend', 'this', -4) {'strength': 1.908, 'spread': 71.09, 'peak': 19.531, 'count': 28}   \n",
        "> ('depend', 'this', -2) {'strength': 1.908, 'spread': 71.09, 'peak': 19.531, 'count': 22}   \n",
        "> ('depend', 'to', -1) {'strength': 8.419, 'spread': 3941.16, 'peak': 109.979, 'count': 228}   \n",
        "> ('depend', 'type', 3) {'strength': 1.295, 'spread': 213.41, 'peak': 22.309, 'count': 50}   \n",
        "> ('depend', 'upon', 1) {'strength': 4.902, 'spread': 4984.01, 'peak': 98.298, 'count': 239}   \n",
        "> ('depend', 'which', -1) {'strength': 4.379, 'spread': 346.16, 'peak': 43.405, 'count': 66}   \n",
        "> ('depend', 'will', -1) {'strength': 3.784, 'spread': 2250.05, 'peak': 68.935, 'count': 159}   \n",
        "> ('depend', 'would', -1) {'strength': 1.601, 'spread': 412.44, 'peak': 29.709, 'count': 70}   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bMqATOZpWEG"
      },
      "source": [
        "## Strongest Collocation\n",
        "There are too many collocations to check your result easily. Hence, we want you use the rules below to find out one strongest collocation for \"depend\".\n",
        "\n",
        "Rule:\n",
        "1. find the collocate with maximum **`strength`** value\n",
        "2. find the collocate with maximum **`count`** value\n",
        "\n",
        "If there're more than two collocations sharing same maximum `strength` value, please use rule 2 to find one as the answer. Otherwise, you can ignore Rule 2.\n",
        "\n",
        "<font color=\"red\">**[ TODO ]**</font> Please find out the strongest collocation for \"depend\" by the rules.\n",
        "\n",
        "The ouput format sholud be `(base word, collocate, distance)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "idhTENjepWEG"
      },
      "outputs": [],
      "source": [
        "def find_strongest_collocation(base_word, filtered_by_C3):\n",
        "### [TODO]\n",
        "    max, max_str, max_count = None, 0, 0\n",
        "    for i in filtered_by_C3:\n",
        "        if filtered_by_C3[i][\"strength\"] > max_str: max, max_str, max_count = i, filtered_by_C3[i][\"strength\"], filtered_by_C3[i][\"count\"]\n",
        "        elif filtered_by_C3[i][\"strength\"] == max_str:\n",
        "            if filtered_by_C3[i][\"count\"] > max_count:\n",
        "                max, max_str, max_count = i, filtered_by_C3[i][\"strength\"], filtered_by_C3[i][\"count\"]\n",
        "    print(max)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dPxhbWqYpWEG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('depend', 'on', 1)\n"
          ]
        }
      ],
      "source": [
        "### Run and Print\n",
        "find_strongest_collocation(base_word, filtered_by_C3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQFst4LLpWEG"
      },
      "source": [
        "<font color=\"green\">Expected output: </font>\n",
        "\n",
        "> ('depend', 'on', 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ub8AdaebpWEG"
      },
      "source": [
        "## Find Helpful AKL Collocation\n",
        "Only one example cannot express how amazing what we just did, so here are some other AKL verbs selected for you to experience. \n",
        "\n",
        "<font color=\"red\">**[ TODO ]**</font> Please finish **combination** function to combine last four functions together and use it to find out strongest collocations for **AKL_verbs**. \n",
        "\n",
        "The ouput format sholud be `(base word, collocate, distance)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fPZ6ulIJpWEG"
      },
      "outputs": [],
      "source": [
        "AKL_verbs = ['argue', 'can', 'consist', 'contrast', 'favour', 'lack', 'may', \n",
        "            'neglect', 'participate', 'present', 'rely', 'suggest']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Yolxm1hfpWEG"
      },
      "outputs": [],
      "source": [
        "def combination(base_word):\n",
        "### [TODO]\n",
        "    c1 = C1_filter(base_word)\n",
        "    c2 = C2_filter(base_word, c1)\n",
        "    c3 = C3_filter(base_word, c2)\n",
        "    find_strongest_collocation(base_word, c3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "31okIDBMpWEG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('argue', 'that', 1)\n",
            "('can', 'be', 1)\n",
            "('consist', 'of', 1)\n",
            "('contrast', 'in', -1)\n",
            "('favour', 'of', 1)\n",
            "('lack', 'of', 1)\n",
            "('may', 'be', 1)\n",
            "('neglect', 'of', 1)\n",
            "('participate', 'in', 1)\n",
            "('present', 'with', -3)\n",
            "('rely', 'on', 1)\n",
            "('suggest', 'that', 1)\n"
          ]
        }
      ],
      "source": [
        "### Run and Print\n",
        "for base_word in AKL_verbs:\n",
        "    combination(base_word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-laz0HgapWEH"
      },
      "source": [
        "<font color=\"green\">Expected output: </font>\n",
        "\n",
        "> ('argue', 'that', 1)   \n",
        "> ('can', 'be', 1)   \n",
        "> ('consist', 'of', 1)   \n",
        "> ('contrast', 'in', -1)   \n",
        "> ('favour', 'of', 1)   \n",
        "> ('lack', 'of', 1)   \n",
        "> ('may', 'be', 1)   \n",
        "> ('neglect', 'of', 1)   \n",
        "> ('participate', 'in', 1)   \n",
        "> ('present', 'with', -3)   \n",
        "> ('rely', 'on', 1)   \n",
        "> ('suggest', 'that', 1)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XO64hc-OpWEH"
      },
      "source": [
        "## TA's Notes\n",
        "\n",
        "If you complete the Assignment, please use [this link](https://docs.google.com/spreadsheets/d/1QGeYl5dsD9sFO9SYg4DIKk-xr-yGjRDOOLKZqCLDv2E/edit#gid=206119035) to reserve demo time.  \n",
        "The score is only given after TAs review your implementation, so <u>**make sure you make a appointment with a TA before you miss the deadline**</u> .  <br>After demo, please upload your assignment to eeclass. You just need to hand in this ipynb file and rename it as XXXXXXXXX(Your student ID).ipynb.\n",
        "<br>Note that **late submission will not be allowed**.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oayUKgdNpWEH"
      },
      "source": [
        "## Reference\n",
        "[Frank Smadja, Retrieving Collocations from Texts: Xtract, Computational Linguistics, Volume 19, 1993](https://aclanthology.org/J93-1007.pdf)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Assignment6.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "f310d5938d55ac9aae2dff15932e2ada3b2a73d3b908146e61496592c896572c"
    },
    "kernelspec": {
      "display_name": "Python 3.7.11 64-bit ('nlp': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
