{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Assignment 01: N-gram Frequency\n",
    "\n",
    "During the natural language processing, sometimes we would like to know how many times a word, a phrase or a pattern has appeared in the text. This is called the **word/phrase frequency** .  \n",
    "For example, in the sentence *\\\"The more you read, the better your vocabulary becomes\\\"*, the fequency of *read* is `1`, while the frequency of *the* is `2` .  \n",
    "\n",
    "Word frequency can be used to gain some knowledge from the article.  \n",
    "As an example, if the frequency of the word *language* is much higher than other words in an article, we can assume that this article may be related to linguistic topics.  \n",
    "\n",
    "In this assignment, we will tell you what N-gram is and how to calculate N-gram frequency.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenization\n",
    "\n",
    "Before we start counting words, of course we need to transform the sentence into *words* .  \n",
    "As what we want, **Tokenization** is the process to split a sentence into smaller word units, which we call *tokens* .  \n",
    "\n",
    "Take the sentence *\"Of course we're celebrating the New Year!\"* for example.  \n",
    "We need to break it into a token list like `[\"Of\", \"course\", ...]` to begin counting.  \n",
    "\n",
    "In English, this could be easy because we can intuitively use spaces to seperate every words.  \n",
    "> \"Of course we're celebrating the New Year!\"  \n",
    "> -> [\"Of\", \"course\", <u>\"we're\"</u>, \"celebrating\", \"the\", \"New\", <u>\"Year!\"</u>]\n",
    "\n",
    "However, some tokenizer may also treat punctuations and abbreviations as independant units:  \n",
    "> \"Of course we're celebrating the New Year!\"  \n",
    "> -> [\"Of\", \"course\", <span style=\"color: red\"><u>\"we\", \"'\", \"re\"</u></span>, \"celebrating\", \"the\", \"New\", <span style=\"color: red\"><u>\"Year\", \"!\"</u></span>]\n",
    "\n",
    "Also, you might want to treat some special terms as single tokens:  \n",
    "> \"Of course we're celebrating the New Year!\"  \n",
    "> -> [\"Of\", \"course\", \"we're\", \"celebrating\", \"the\", <span style=\"color: red\"><u>\"New Year!\"</u></span>]\n",
    "\n",
    "And in Chinese this becomes even trickier without the hint from spaces:  \n",
    "> \"今天天氣真好。\"  \n",
    "> -> [\"今天\", \"天氣\", \"真\", \"好\", \"。\"]\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Knowing what tokenization is, now we can start to build our own counter!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**open the file**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "import re"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "file_path = os.path.join('/home/stanley/Desktop/VSLab/Course/NLPlab/20210916', 'big.txt') # change to where you put your data\n",
    "with open(file_path, 'r') as f:\n",
    "    text = f.read()\n",
    "print(text[:90])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The Project Gutenberg EBook of The Adventures of Sherlock Holmes\n",
      "by Sir Arthur Conan Doyle\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**<span style=\"color: red\">[ TODO ]</span> Implement your tokenization function here!**  \n",
    "\n",
    "In this assignment, we will use the following rules: \n",
    " 1. Ignore case (e.g., \"The\" is the same as \"the\")\n",
    " 2. Split by white spaces and punctuations\n",
    " 3. Ignore all punctuation\n",
    "\n",
    "Example:\n",
    "`\"Hello! I'm your TA!\" -> [\"hello\", \"i\", \"m\", \"your\", \"ta\"]`  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def tokenize(text):\n",
    "    # [ TODO ] transform to lower case\n",
    "    text = text.lower()\n",
    "    # [ TODO ] seperate the words\n",
    "    tokens = re.findall(r\"[\\w]+\", text)\n",
    "    return tokens"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Test your function!**  \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "tokens = tokenize(text)\n",
    "print(tokens[:10])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['the', 'project', 'gutenberg', 'ebook', 'of', 'the', 'adventures', 'of', 'sherlock', 'holmes']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<span style=\"color: green\">Expected output:</span> `['the', 'project', 'gutenberg', 'ebook', 'of', 'the', 'adventures', 'of', 'sherlock', 'holmes']`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Frequency counting\n",
    "We have splitted the sentence into tokens! Now we can try to calculate the frequency.  \n",
    "<span style=\"color: red\">[ TODO ]</span> Try to <u>write a counter to sum up how many times the word shows up</u>.   \n",
    "<small>(*Hint: dict, defaultdict, counter, etc.)</small>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def calculate_frequency(tokens):\n",
    "    # [ TODO ]\n",
    "    frequency = {}\n",
    "    for t in tokens:\n",
    "        try:\n",
    "            frequency[t] += 1\n",
    "        except:\n",
    "            frequency[t] = 1\n",
    "    \"\"\"\n",
    "    Sample output: \n",
    "    {\n",
    "        'the': 79809, \n",
    "        'project': 288,\n",
    "        ...\n",
    "    }\n",
    "    \"\"\"\n",
    "    return frequency"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "counter = calculate_frequency(tokens)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since there are too many entries in this counter, it may not be a good idea if we print them all.  \n",
    "<span style=\"color: red\">[ TODO ]</span> Let's <u>write an utility function to print only the words with top-N frequency</u>. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def print_top_n(counter, n = 10):\n",
    "    top_n = [(counter[t], t) for t in counter]\n",
    "    for i in sorted(top_n, reverse=True)[:n]:\n",
    "        print(\"{} {}\".format(i[1], i[0]))\n",
    "    \n",
    "    return"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "print_top_n(counter, n=10)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "the 79809\n",
      "of 40024\n",
      "and 38312\n",
      "to 28765\n",
      "in 22023\n",
      "a 21124\n",
      "that 12512\n",
      "he 12401\n",
      "was 11410\n",
      "it 10681\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<span style=\"color: green\">Expected output:</span>\n",
    "```\n",
    "the 79809\n",
    "of 40024\n",
    "and 38312\n",
    "to 28765\n",
    "in 22023\n",
    "a 21124\n",
    "that 12512\n",
    "he 12401\n",
    "was 11410\n",
    "it 10681\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## N-gram\n",
    "\n",
    "Now we have known how to calculate the word frequency.  \n",
    "However, sometimes we not only want to know about single words, but also about the *phrases*, and here N-gram can be brought in.  \n",
    "\n",
    "**N-gram is a contiguous sequence of n items from a given sample of text or speech.** <small>(Definition from [wikipedia](https://en.wikipedia.org/wiki/N-gram))</small>  \n",
    "\n",
    "Consider the token list from previous exmaple: `[\"Of\", \"course\", \"we're\", \"celebrating\", \"the\", \"New\", \"Year!\"]` .  \n",
    "The 2-gram, or bigram, of this example is `[\"Of course\", \"course we're\", \"we're celebrating\", ...]` .  \n",
    "You may notice that the phrase \"of course\" now is bundled and counted together, and this is where N-gram has its power.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<span style=\"color: red\">[ TODO ]</span> In the following section, let's (1) <u>write a function to generate n-gram</u> and then (2) <u>calculate the frequency of grams</u> with different width.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def get_ngram(tokens, n=2):\n",
    "    tokens = [tokens[i:i+n] for i in range(len(tokens)) if (i+n-1) <= len(tokens) - 1]\n",
    "    tokens = [' '.join(t) for t in tokens]\n",
    "    \n",
    "    return tokens"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "bigram = get_ngram(tokens, n=2)\n",
    "print(bigram[:5])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['the project', 'project gutenberg', 'gutenberg ebook', 'ebook of', 'of the']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<span style=\"color: green\">Expected output:</span> `['the project', 'project gutenberg', 'gutenberg ebook', 'ebook of', 'of the']`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "After the 2-gram is generated, we can use the same counter we built above to count the frequency.  \n",
    "<small>Note that if your counter couldn't work as expected here, you need to fix it above.</small>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "bigram_counter = calculate_frequency(bigram)\n",
    "print_top_n(bigram_counter)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "of the 12528\n",
      "in the 6446\n",
      "to the 4464\n",
      "and the 3202\n",
      "on the 2526\n",
      "at the 2103\n",
      "by the 1937\n",
      "from the 1865\n",
      "with the 1735\n",
      "of a 1710\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<span style=\"color: green\">Expected output:</span>\n",
    "```\n",
    "of the 12528\n",
    "in the 6446\n",
    "to the 4464\n",
    "and the 3202\n",
    "on the 2526\n",
    "at the 2103\n",
    "by the 1937\n",
    "from the 1865\n",
    "with the 1735\n",
    "of a 1710\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "\n",
    "Congraturation! You've succefully built a simple ngram frequency calculator! ;)  \n",
    "Feel free to try building a trigram, 4-gram, etc. counter and observe the difference.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TA's Notes\n",
    "\n",
    "Assignment#1 is just a warm-up practice, so you don't need to hand it in this week.  \n",
    "However, **starting from the next week you'll need explain your implementation to TAs** after you finish your assignment.  \n",
    "The score is only given after TAs review your implementation, so <u>**make sure you make a appointment with TA before you miss the deadline**</u> .  \n",
    "Note that **late submission will not be allowed**.  "
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f8407c4e4ad4e83fbda062738558f90c8a1755d9eaef2b10bf00fe3f026ecca1"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('nlp': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}