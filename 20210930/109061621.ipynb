{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Assignment 03: Simple Language Model\n",
    "\n",
    "This week, we want you to create a Language Model (LM) to judge how common a sentence is.  \n",
    "More specificly, you need to <u>calculate the probability of a given setnence</u> showing in an article.  \n",
    "\n",
    "*No need to make your output exactly the same as ours. As long as it's *reasonable* (which means you can explain it), you get your points.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## What is a Language Model?\n",
    "\n",
    "<b>A statistical language model is a probability distribution over sequences of words. Given such a sequence, say of length $m$, it assigns a probability $P(w_1, \\ldots ,w_m)$ to the whole sequence.</b>\n",
    "<i>-- from <a href=\"https://en.wikipedia.org/wiki/Language_model\">Wikipedia</a></i>  \n",
    "\n",
    "To put it simply, Language Model calculates the probability of a word, a sequence of words, or a sentence.  \n",
    "This can be useful in many NLP tasks, like machine translation, spelling checking, speech recognition, etc.  \n",
    "\n",
    "Consider the following two sentence  \n",
    "\n",
    "> (1) He is looking <font color=\"green\">*to*</font> a new job.  \n",
    "\n",
    "and \n",
    "\n",
    "> (2) He is looking <font color=\"green\">*for*</font> a new job.  \n",
    "\n",
    "\n",
    "We can see that there's a grammar error in sentence 1, for which the LM should generate a lower probability.  \n",
    "Hence, if your LM is well-established, you are able to judge which sentence is more correct (or more common) among a set of candidate sentences.  \n",
    "\n",
    "But how to do so?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Implementation\n",
    "\n",
    "### Step 1 - Ngram frequency\n",
    "\n",
    "First of all, we need to calculate 1- and 2-gram frequency from the corpus.  \n",
    "**Again**, yes. As we have said in the first class, word/phrase frequency plays an important role in NLP.\n",
    "\n",
    "You should be familiar with this procedure now, so let's skip the boring explanation.  \n",
    "<small>*Please refer to assignment 1 and 2 if you have any question.</small>  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "with open(os.path.join('data', 'big.txt'), 'r') as f:\n",
    "  corpus = f.read()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color=\"red\">**[ TODO ]**</font> Calculate 1- and 2-gram frequency with the given corpus."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def unigram(text):\n",
    "  \"\"\" [TODO] Get the 1-gram list from string\n",
    "  @param text: given string\n",
    "  @return: a list of token\n",
    "  \"\"\"\n",
    "  text = text.lower()\n",
    "  tokens = re.findall(r\"[\\w]+\", text)\n",
    "\n",
    "  return tokens\n",
    "\n",
    "def bigram(text):\n",
    "  \"\"\" [TODO] Get the 2-gram list \"\"\"\n",
    "  text, n = text.lower(), 2\n",
    "  tokens = re.findall(r\"[\\w]+\", text)\n",
    "  tokens = [tokens[i:i+n] for i in range(len(tokens)) if (i+n-1) <= len(tokens) - 1]\n",
    "  tokens = [' '.join(t) for t in tokens]\n",
    "\n",
    "  return tokens\n",
    "\n",
    "def calculate_frequency(tokens):\n",
    "    frequency = {}\n",
    "    for t in tokens:\n",
    "      try:\n",
    "        frequency[t] += 1\n",
    "      except:\n",
    "        frequency[t] = 1\n",
    "            \n",
    "    return frequency\n",
    "\n",
    "uni_freq = calculate_frequency(unigram(corpus)) # [TODO] calculate the 1-gram frequency of the corpus\n",
    "bi_freq = calculate_frequency(bigram(corpus)) # [TODO] calculate the 2-gram frequency of the corpus"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 2: Probability of a word\n",
    "\n",
    "#### Unigram  \n",
    "Probability of a word, or a *unigram*, is simple: its occurence devided by the total word count should do the trick.  \n",
    "$$P(w) = \\frac{Freq(w)}{N}$$  \n",
    ", where $N = $ count of all words.  \n",
    "But the unigram language model doesn't consider other context words, so we choose to use bigram model here.  \n",
    "\n",
    "#### Bigram  \n",
    "As to 2-gram probability, we would use the Maximum Likelihood Estimate (MLE) to calculate it.  \n",
    "\n",
    "$$P_{mle}(w_i|w_{i-1}) = \\frac{Freq(w_{i-1}, w_i)}{Freq(w_{i-1})}$$\n",
    "\n",
    "For example, if we have a corpus with \n",
    "\n",
    "> can you tell me about any good cantonese restaurants  \n",
    "> tell me about chez panisse  \n",
    "> can you give me a listing of the kinds of food that are available  \n",
    "\n",
    "Then the probability of *tell* preceding *me* is $P_{mle}(me|tell) = \\frac{2}{2} = 1$.  \n",
    "Similarly, the probability of *me* preceding *about* is $P_{mle}(about|me) = \\frac{2}{3}$.  \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color=\"red\">**[ TODO ]**</font> Calculate the bigram probability."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def P_mle(uni_freq, bi_freq, word: str, pre_word: str = None):\n",
    "  \"\"\" [TODO] Return the P(word|pre_word), which should be a float \"\"\"\n",
    "  if pre_word == None:\n",
    "    N = sum(uni_freq.values())\n",
    "    p_w = uni_freq[word] / N\n",
    "  else:\n",
    "    w = ' '.join((pre_word, word))\n",
    "    try:\n",
    "      p_w = bi_freq[w] / uni_freq[pre_word]\n",
    "    except:\n",
    "      p_w = 0 / uni_freq[pre_word]\n",
    "        \n",
    "  return p_w"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "p1 = P_mle(uni_freq, bi_freq, 'book', 'read')\n",
    "p2 = P_mle(uni_freq, bi_freq, 'books', 'read')\n",
    "print(f'p1: {p1}; \\np2: {p2}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "p1: 0.0; \n",
      "p2: 0.0045662100456621\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color=\"green\">Expected output: </font> `p1` should be **smaller** than `p2` (expetced to be `0`).  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "p1 = P_mle(uni_freq, bi_freq, 'books', 'a')\n",
    "p2 = P_mle(uni_freq, bi_freq, 'book', 'a')\n",
    "print(f'p1: {p1}; \\np2: {p2}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "p1: 0.0; \n",
      "p2: 0.0010414694186707063\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color=\"green\">Expected output: </font> `p1` should be **smaller** than `p2` (expetced to be `0`).  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "p1 = P_mle(uni_freq, bi_freq, 'have', 'he')\n",
    "p2 = P_mle(uni_freq, bi_freq, 'has', 'he')\n",
    "print(f'p1: {p1}; \\np2: {p2}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "p1: 0.0006451092653818241; \n",
      "p2: 0.015240706394645594\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color=\"green\">Expected output: </font> `p1` should be **smaller** than `p2`.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 3 - Sentence probability\n",
    "\n",
    "Now we have the probability of every word and bigram. But how should we combine them and get the probability of a sentence?  \n",
    "\n",
    "Recall the chain rule in probability: \n",
    "$$P(X_1, X_2, X_3, X_4) = P(X_1) \\cdot P(X_2|X_1) \\cdot P(X_3|X_1,X_2) \\cdot P(X_4|X_1, X_2, X_3)$$  \n",
    "And with Markove Assumption, we know that \n",
    "$$P(w_1, w_2, \\dots, w_n) \\approx P(w_2|w_1) \\cdot P(w_3|w_2) \\cdot \\dots \\cdot P(w_n|w_{n-1})$$\n",
    "\n",
    "For example,\n",
    "$$P(I\\:want\\:a\\:cake) \\approx P(want|I) \\cdot P(a|want) \\cdot P(cake|a)$$\n",
    "\n",
    "Since we have already built a method to calculate $P(w_{i}|w_{i-1})$, we can get the probability of a sentence through this equation.  \n",
    "Note that it's recommended to sum them under $log$-scale to prevent underflow, because gram probabilities are mostly some small floating points, \n",
    "$$log(p_1 \\cdot p_2 \\dots p_n) = log(p_1) + log(p_2) + \\dots + log(p_n) $$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color=\"red\">**[ TODO ]**</font> Calculate the sentence probability. (Please do so under the `log` scale to prevent underflow)  \n",
    "<small>*Hint: `math`</small>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "def sentence_prob(uni_freq, bi_freq, sentence: str, P):\n",
    "  \"\"\" Calculate the probability of a given sentence with given P-model.\n",
    "  @param sentence: the given sentence\n",
    "  @param P: the probability model to use. (like your `P_mle` above)\n",
    "  @return: a float indicating the probability\n",
    "  \"\"\"\n",
    "  words = unigram(sentence)\n",
    "  sen_prob = 0\n",
    "  for i in range(1, len(words)):\n",
    "    p = P(uni_freq, bi_freq, words[i], words[i-1])\n",
    "    # print(\"P({}|{}) = {}\".format(words[i], words[i-1], a))\n",
    "    sen_prob += np.log(p + 1e-20)\n",
    "\n",
    "  return sen_prob"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "p1 = sentence_prob(uni_freq, bi_freq, 'He have to take the medicine.', P=P_mle)\n",
    "p2 = sentence_prob(uni_freq, bi_freq, 'He has to take the medicine.', P=P_mle)\n",
    "print(f'p1: {p1}; \\np2: {p2}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "p1: -28.51828771819755; \n",
      "p2: -26.03750048589407\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color=\"green\">Expected output: </font> `p1` should be **smaller** than `p2`.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Looks good so far?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "However, this model heavily depends on how large your dataset is.   \n",
    "If you give it the word or gram not existing in the corpus, the probability of it will be $0$, which causes problems during the calculation (because you can't divide numbers by $0$).  \n",
    "\n",
    "Try this! "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "sentence_prob(uni_freq, bi_freq, \"He has to eat medicine.\", P=P_mle)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "-61.32810063620518"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "See? Now you get the problem.  \n",
    "Surely you can expand your dataset to decrease the number of unseen phrases, but it's impossible to cover all phrases in the world.  \n",
    "Here the smoothing technique comes to rescue!  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 4 - smoothing\n",
    "\n",
    "A naÃ¯ve solution is to set the default frequency as `1`.  \n",
    "For example, if the original frequency is\n",
    "\n",
    "> hello: 2  \n",
    "> world: 1  \n",
    "> empty: 0  \n",
    "> null: 0  \n",
    "\n",
    ", after defaulting the frequency to 1, it would be\n",
    "\n",
    "> hello: 3  \n",
    "> world: 2  \n",
    "> empty: <font color=\"red\">**1**</font><br/>\n",
    "> null: <font color=\"red\">**1**</font><br/>\n",
    "\n",
    "\n",
    "And if the frequency is all added by 1, the probability of bigram will now be  \n",
    "$$ P_{add1} = \\frac{Freq(w_{i-1}, w_i)+1}{Freq(w_{i-1})+N}$$\n",
    ", where $N = count\\:of\\:all\\:tokens$ .  \n",
    "\n",
    "This is called the **Add-1 Estimation**, or also **Laplace Smoothing**.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color=\"red\">**[ TODO ]**</font> Implement the Laplace smoothing.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "def P_add1(uni_freq, bi_freq, word: str, pre_word: str):\n",
    "  \"\"\" [TODO] Return the P(word|pre_word) with Laplace smoothing. \"\"\"\n",
    "  w = ' '.join((pre_word, word))\n",
    "  try:\n",
    "    p_w = (bi_freq[w] + 1) / (uni_freq[pre_word] + sum(uni_freq.values()))\n",
    "  except:\n",
    "    p_w = 1 / (uni_freq[pre_word] + sum(uni_freq.values()))\n",
    "\n",
    "  return p_w"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "p1 = P_add1(uni_freq, bi_freq, 'medicine', 'eat')\n",
    "p2 = P_add1(uni_freq, bi_freq, 'medicine', 'take')\n",
    "print(f'p1: {p1}; \\np2: {p2}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "p1: 8.963472951375849e-07; \n",
      "p2: 1.7917919801182761e-06\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color=\"green\">Expected output:</font> `p1` shouldn't throw any error, and `p1` should be **smaller** than `p2`. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, Add-1 estimation is a really naive method and doesn't always produce good results.  \n",
    "There are better smooting techiniques like <a href=\"https://en.wikipedia.org/wiki/Good%E2%80%93Turing_frequency_estimation\">Good-Turing Estimation</a> or <a href=\"https://en.wikipedia.org/wiki/Kneser%E2%80%93Ney_smoothing\">Kneser-Ney Smoothing</a>. Feel free to give it a try!  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test your LM\n",
    "\n",
    "Seems that you've done a great job! Let's give it some tests to see if it works as expected.  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "test_cases = [\n",
    "  'He have to take the medicine.',\n",
    "  'He has to eat the medicine.',\n",
    "  'He has to take the medicine.'\n",
    "]\n",
    "\n",
    "for sent in test_cases:\n",
    "  print(sent, sentence_prob(uni_freq, bi_freq, sent, P=P_add1))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "He have to take the medicine. -52.138865978097584\n",
      "He has to eat the medicine. -56.0647163911012\n",
      "He has to take the medicine. -50.51892711787963\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color=\"green\">Expected output:</font> The last sentence should return the highest probability. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "test_cases = [\n",
    "  'He is looking to a job.',\n",
    "  'He is looking for a job.'\n",
    "]\n",
    "\n",
    "for sent in test_cases:\n",
    "  print(sent, sentence_prob(uni_freq, bi_freq, sent, P=P_add1))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "He is looking to a job. -53.15800737444616\n",
      "He is looking for a job. -52.05143935375931\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<font color=\"green\">Expected output:</font> The second sentence should return a higher probability. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Think about it\n",
    "Think about the following questions. **TA will ask your answers during the demo.**  \n",
    "\n",
    "1. What would happen if you use MLE Estimation to compare two sentences with different lengthes?  \n",
    "   Examples. (1) *He school nice and happy.* and (2) *He went to school yesterday and had a nice time there.*  \n",
    "\n",
    "2. According to Q1, Do you think MLE model is fair? If so, why? If not, how could you improve this? (Just think about it; no need to actually implement it)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Congratulation!** You've successfully built your own language model.  \n",
    "You should have caught a glimpse of LM's power despite its plainness.  \n",
    "Feel free to play around with your LM and improve it, like  \n",
    "1. using a larger dataset, \n",
    "2. applying better smoothing technique, or \n",
    "3. considering longer gram dependency (3- to 5-gram are resonable length).  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TA's note\n",
    "\n",
    "\n",
    "Remember to <b><a href=\"https://docs.google.com/spreadsheets/d/1QGeYl5dsD9sFO9SYg4DIKk-xr-yGjRDOOLKZqCLDv2E/edit?usp=sharing\">make an appoiment with TA</a> to demo/explain your implementation <u>before <font color=\"red\">10/7 15:30</font></u></b> .  \n",
    "You should also save your file as {student_id}.ipynb and submit it to <a href=\"https://eeclass.nthu.edu.tw/course/homework/2384\">eeclass</a> .  \n",
    "**Late submission is not allowed.**  "
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f8407c4e4ad4e83fbda062738558f90c8a1755d9eaef2b10bf00fe3f026ecca1"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('nlp': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}